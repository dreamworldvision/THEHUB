import os
import argparse
from dotenv import load_dotenv
from gtts import gTTS
from moviepy.editor import *
from moviepy.video.fx.all import fadein, fadeout
import requests
import openai
import numpy as np

# Load environment variables
load_dotenv()
STABILITY_API_KEY = os.getenv("STABILITY_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

def generate_image(prompt, engine="stability"):
    """Generate image using either Stability AI or DALL-E"""
    if engine == "stability":
        headers = {"Authorization": f"Bearer {STABILITY_API_KEY}"}
        data = {
            "text_prompts": [{"text": prompt}],
            "height": 512,
            "width": 512,
            "steps": 30
        }
        response = requests.post(
            "https://api.stability.ai/v1/generation/stable-diffusion-512-v2-1/text-to-image",
            headers=headers,
            json=data
        )
        if response.status_code == 200:
            return response.content
        raise Exception(f"Stability API Error: {response.text}")
    
    elif engine == "dalle":
        openai.api_key = OPENAI_API_KEY
        response = openai.Image.create(
            prompt=prompt,
            n=1,
            size="512x512"
        )
        img_url = response['data'][0]['url']
        return requests.get(img_url).content

def text_to_speech(text, output_path="temp_audio.mp3"):
    """Convert text to speech using gTTS"""
    tts = gTTS(text=text, lang='en', slow=False)
    tts.save(output_path)
    return output_path

def create_video_clip(image_content, audio_path, sentence, duration, add_transitions=True, add_subtitles=True):
    """Create video clip with transitions and subtitles"""
    # Save image to temp file
    with open("temp_image.png", "wb") as f:
        f.write(image_content)
    
    # Create image clip
    clip = ImageClip("temp_image.png").set_duration(duration)
    
    # Add transitions
    if add_transitions:
        clip = clip.fx(fadein, 0.5).fx(fadeout, 0.5)
    
    # Add audio
    audio = AudioFileClip(audio_path)
    clip = clip.set_audio(audio)
    
    # Add subtitles
    if add_subtitles:
        txt_clip = TextClip(sentence, fontsize=24, color='white', font='Arial', stroke_color='black', stroke_width=1)
        txt_clip = txt_clip.set_position(('center', 'bottom')).set_duration(duration)
        clip = CompositeVideoClip([clip, txt_clip])
    
    return clip

def add_background_music(video_clip, music_path="background_music.mp3"):
    """Add background music to video"""
    if os.path.exists(music_path):
        music = AudioFileClip(music_path).volumex(0.3)
        new_audio = CompositeAudioClip([video_clip.audio, music])
        return video_clip.set_audio(new_audio)
    return video_clip

def text_to_video(
    text,
    output_file="output.mp4",
    engine="stability",
    add_transitions=True,
    add_subtitles=True,
    background_music=None
):
    """Main conversion function with enhanced features"""
    # Split text into sentences
    sentences = [s.strip() for s in text.split('.') if s.strip()]
    
    clips = []
    
    for sentence in sentences:
        try:
            # Generate assets for each sentence
            image_content = generate_image(sentence, engine=engine)
            audio_path = text_to_speech(sentence)
            
            # Get audio duration
            audio = AudioFileClip(audio_path)
            duration = audio.duration
            
            # Create clip
            clip = create_video_clip(
                image_content,
                audio_path,
                sentence,
                duration,
                add_transitions=add_transitions,
                add_subtitles=add_subtitles
            )
            clips.append(clip)
        except Exception as e:
            print(f"Error processing sentence '{sentence}': {str(e)}")
    
    # Combine all clips
    final_video = concatenate_videoclips(clips)
    
    # Add background music
    if background_music:
        final_video = add_background_music(final_video, background_music)
    
    # Write final video
    final_video.write_videofile(output_file, fps=24, codec='libx264', audio_codec='aac')
    
    # Cleanup temp files
    for f in ["temp_image.png", "temp_audio.mp3"]:
        if os.path.exists(f):
            os.remove(f)
    
    return output_file

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Convert text to video")
    parser.add_argument("--text", type=str, required=True, help="Input text")
    parser.add_argument("--output", type=str, default="output.mp4", help="Output video file")
    parser.add_argument("--engine", choices=["stability", "dalle"], default="stability", help="Image generation engine")
    parser.add_argument("--no-transitions", action="store_false", dest="transitions", help="Disable transitions")
    parser.add_argument("--no-subtitles", action="store_false", dest="subtitles", help="Disable subtitles")
    parser.add_argument("--background-music", type=str, help="Path to background music file")
    
    args = parser.parse_args()
    
    result = text_to_video(
        args.text,
        output_file=args.output,
        engine=args.engine,
        add_transitions=args.transitions,
        add_subtitles=args.subtitles,
        background_music=args.background_music
    )
    print(f"Video created: {result}")